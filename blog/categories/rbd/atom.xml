<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: RBD | CephNotes]]></title>
  <link href="http://cephnotes.ksperis.com/blog/categories/rbd/atom.xml" rel="self"/>
  <link href="http://cephnotes.ksperis.com/"/>
  <updated>2017-10-03T12:34:10+02:00</updated>
  <id>http://cephnotes.ksperis.com/</id>
  <author>
    <name><![CDATA[Laurent Barbe]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[LXC 2.0.0 First support for Ceph RBD]]></title>
    <link href="http://cephnotes.ksperis.com/blog/2016/04/14/lxc-2-dot-0-0-first-support-for-ceph-rbd/"/>
    <updated>2016-04-14T14:10:12+02:00</updated>
    <id>http://cephnotes.ksperis.com/blog/2016/04/14/lxc-2-dot-0-0-first-support-for-ceph-rbd</id>
    <content type="html"><![CDATA[<p>FYI, the first RBD support has been added to LXC commands.</p>

<h2>Example :</h2>

<p>```bash</p>

<h1>Install LXC 2.0.0 (ubuntu) :</h1>

<p>$ add-apt-repository ppa:ubuntu-lxc/lxc-stable
$ apt-get update
$ apt-get install lxc</p>

<h1>Add a ceph pool for lxc bloc devices :</h1>

<p>$ ceph osd pool create lxc 64 64</p>

<h1>To create the container, you only need to specify &ldquo;rbd&rdquo; backingstore :</h1>

<p>$ lxc-create -n ctn1 -B rbd -t debian
/dev/rbd0
debootstrap est /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 &hellip;
Copying rootfs to /usr/lib/x86_64-linux-gnu/lxc&hellip;
Generation complete.
```</p>

<p>```
$ rbd showmapped
id pool image snap device  <br/>
0  lxc  ctn1  &ndash;    /dev/rbd0</p>

<p>$ rbd -p lxc info ctn1
rbd image &lsquo;ctn1&rsquo;:</p>

<pre><code>size 1024 MB in 256 objects
order 22 (4096 kB objects)
block_name_prefix: rb.0.1217d.74b0dc51
format: 1
</code></pre>

<p>```</p>

<p><code>
$ lxc-start -n ctn1
$ lxc-attach -n ctn1
ctn1$ mount | grep ' / '
/dev/rbd/lxc/ctn1 on / type ext3 (rw,relatime,stripe=1024,data=ordered)
</code></p>

<p><code>bash
$ lxc-destroy -n ctn1
Removing image: 100% complete...done.
Destroyed container ctn1
</code></p>

<!-- more -->


<h2>Some options :</h2>

<p>```
&mdash;rbdpool POOL   : will create the blockdevice in the pool named POOL, rather than the default, which is &lsquo;lxc&rsquo;.</p>

<p>&mdash;rbdname RBDNAME : will create a blockdevice named RBDNAME rather than the default, which is the container name.</p>

<p>&mdash;fssize : Create a RBD of size SIZE * unit U (bBkKmMgGtT)
```</p>

<p>For example :
<code>bash
$ lxc-create -n ctn2 -B rbd -t debian --rbdpool=rbd --rbdname=lxc-ctn2 --fssize=2G
</code></p>

<h2>What is not yet done:</h2>

<ul>
<li>Persistence on reboot: (RBD can be optionally added to rbdmap file)</li>
<li>Snapshots</li>
<li>Params for authentication (user, keyring&hellip;)</li>
<li>Other rbd options (format, &hellip;)</li>
</ul>


<p>The release annoucement :</p>

<p><a href="https://linuxcontainers.org/fr/lxc/news/#lxc-200-release-announcement-6th-of-april-2016">https://linuxcontainers.org/fr/lxc/news/#lxc-200-release-announcement-6th-of-april-2016</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Use trim/discard with rbd kernel client (since kernel 3.18)]]></title>
    <link href="http://cephnotes.ksperis.com/blog/2014/12/18/use-discard-with-krbd-client-since-kernel-3-dot-18/"/>
    <updated>2014-12-18T17:57:13+01:00</updated>
    <id>http://cephnotes.ksperis.com/blog/2014/12/18/use-discard-with-krbd-client-since-kernel-3-dot-18</id>
    <content type="html"><![CDATA[<p>Realtime :
<code>bash
mount -o discard /dev/rbd0 /mnt/myrbd
</code></p>

<p>Using batch :
<code>bash
fstrim /mnt/myrbd
</code></p>

<!-- more -->


<h2>Test</h2>

<p>The empty FS :</p>

<pre><code>$ rbd create rbd/myrbd --size=20480
$ mkfs.xfs /dev/rbd0
$ rbd diff rbd/myrbd | awk '{ SUM += $2 } END { print SUM/1024/1024 " MB" }'
14.4062 MB
</code></pre>

<p>With a big file&hellip; :</p>

<pre><code>$ mount /dev/rbd0 /mnt/myrbd
$ dd if=/dev/zero of=/mnt/myrbd/testfile bs=1M count=1024
$ rbd diff rbd/myrbd | awk '{ SUM += $2 } END { print SUM/1024/1024 " MB" }'
1038.41 MB
</code></pre>

<p>When the big file has been removed (with file system not mount with &ldquo;discard&rdquo;) :</p>

<pre><code>$ rm /mnt/myrbd/testfile
$ rbd diff rbd/myrbd | awk '{ SUM += $2 } END { print SUM/1024/1024 " MB" }'
1038.41 MB
</code></pre>

<p>Launch FS trim :</p>

<pre><code>$ fstrim /mnt/myrbd
$ rbd diff rbd/myrbd | awk '{ SUM += $2 } END { print SUM/1024/1024 " MB" }'
10.6406 MB
</code></pre>

<h2>Benchmark discard option</h2>

<p>Without &ldquo;discard&rdquo; :</p>

<pre><code>$ mount /dev/rbd0 /mnt/rbd0

$ mkdir testdir; cd testdir
$ dd if=/dev/zero of=mainfile bs=1M count=200
$ split -b 4048 -a 7 mainfile; sync               # 4k file / ~51k files
$ cd ..
$ time rm -rf testdir; time sync
real    0m2.780s
user    0m0.096s
sys     0m2.632s

real    0m0.130s
user    0m0.004s
sys     0m0.016s

# total: &lt; 3s
</code></pre>

<p>With &ldquo;discard&rdquo; :</p>

<pre><code>$ mount -o discard /dev/rbd1 /mnt/rbd1

$ mkdir testdir; cd testdir
$ dd if=/dev/zero of=mainfile bs=1M count=200
$ split -b 4048 -a 7 mainfile; sync               # 4k file / ~51k files
$ cd ..
$ time rm -rf testdir; time sync
real    1m51.471s
user    0m0.104s
sys     0m2.084s

real    0m47.262s
user    0m0.000s
sys     0m0.008s

# total: ~1m56
</code></pre>

<p>This is on 2 osd without ssd journal&hellip;</p>

<p>In the case of intensive use of the file system, with many small file, it may be more advantageous to use fstrim, for example once a day.</p>

<p>(Ceph osd support trim for block device since 0.46)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ceph RBD with LXC containers]]></title>
    <link href="http://cephnotes.ksperis.com/blog/2014/11/17/ceph-rbd-with-lxc-containers/"/>
    <updated>2014-11-17T11:15:19+01:00</updated>
    <id>http://cephnotes.ksperis.com/blog/2014/11/17/ceph-rbd-with-lxc-containers</id>
    <content type="html"><![CDATA[<p><blockquote><p>Update on Apr 14th, 2016:</p><footer><strong>LXC 2.0.0 First Support for Ceph RBD : <a href="http://cephnotes.ksperis.com/blog/2016/04/14/lxc-2-dot-0-0-first-support-for-ceph-rbd">http://cephnotes.ksperis.com/blog/2016/04/14/lxc-2-dot-0-0-first-support-for-ceph-rbd</a></strong></footer></blockquote></p>

<p>A simple way to secure your data with containers is to use a distributed storage such as Ceph for LXC root storage.</p>

<p>For exemple :</p>

<p>``` bash</p>

<h1>lxc-create -n mycontainer -t debian -B rbd &mdash;pool rbd &mdash;rbd mycontainer &mdash;fstype ext4 &mdash;fssize 500</h1>

<p>```</p>

<!-- more -->


<p>```
mke2fs 1.42.5 (29-Jul-2012)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=4096 blocks, Stripe width=4096 blocks
128016 inodes, 512000 blocks
25600 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=67633152
63 block groups
8192 blocks per group, 8192 fragments per group
2032 inodes per group
Superblock backups stored on blocks:</p>

<pre><code>8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409
</code></pre>

<p>Allocating group tables: done                          <br/>
Writing inode tables: done                          <br/>
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done</p>

<p>Note: Usually the template option is called with a configuration
file option too, mostly to configure the network.
For more information look at lxc.conf (5)</p>

<p>debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-wheezy-amd64 &hellip;
Copying rootfs to /var/lib/lxc/mycontainer/rootfs&hellip;Generating locales (this might take a while)&hellip;
  en_US.UTF-8&hellip; done
Generation complete.
update-rc.d: using dependency based boot sequencing
update-rc.d: using dependency based boot sequencing
update-rc.d: using dependency based boot sequencing
update-rc.d: using dependency based boot sequencing</p>

<p>Current default time zone: &lsquo;America/New_York&rsquo;
Local time is now:      Tue Nov 18 09:34:16 EST 2014.
Universal Time is now:  Tue Nov 18 14:34:16 UTC 2014.</p>

<p>Root password is &lsquo;root&rsquo;, please change !
&lsquo;debian&rsquo; template installed
&lsquo;mycontainer&rsquo; created
```</p>

<p>```</p>

<h1>mount | grep mycontainer</h1>

<p>/dev/rbd1 on /var/lib/lxc/mycontainer/rootfs type ext4 (rw,relatime,stripe=4096,data=ordered)
```</p>

<p>Diff file for lxc-create :</p>

<p>``` bash</p>

<h1>diff -u /usr/bin/lxc-create.orig /usr/bin/lxc-create</h1>

<p>```</p>

<p>``` diff
&mdash;&ndash; /usr/bin/lxc-create.orig    2014-11-17 04:16:41.181942000 -0500
+++ /usr/bin/lxc-create 2014-11-17 04:35:27.225942000 -0500
@@ -24,6 +24,7 @@</p>

<pre><code> echo "usage: lxc-create -n &lt;name&gt; [-f configuration] [-t template] [-h] [fsopts] -- [template_options]"
 echo "   fsopts: -B none"
 echo "   fsopts: -B lvm [--lvname lvname] [--vgname vgname] [--fstype fstype] [--fssize fssize]"
</code></pre>

<ul>
<li> echo &ldquo;   fsopts: -B rbd [&mdash;pool poolname] [&mdash;rbd rbd] [&mdash;fstype fstype] [&mdash;fssize fssize]&rdquo;
 echo &ldquo;   fsopts: -B btrfs&rdquo;
 echo &ldquo;           flag is not necessary, if possible btrfs support will be used&rdquo;

<h1>echo &ldquo;   fsopts: -B union [&mdash;uniontype overlayfs]&rdquo;</h1>

<p>@@ -64,7 +65,7 @@
}</p></li>
</ul>


<p> shortoptions=&lsquo;hn:f:t:B:&rsquo;
-longoptions=&lsquo;help,name:,config:,template:,backingstore:,fstype:,lvname:,vgname:,fssize:&rsquo;
+longoptions=&lsquo;help,name:,config:,template:,backingstore:,fstype:,lvname:,vgname:,pool:,rbd:,fssize:&rsquo;
 localstatedir=/var
 lxc_path=${localstatedir}/lib/lxc
 bindir=/usr/bin
@@ -119,6 +120,16 @@</p>

<pre><code>    vgname=$1
    shift
    ;;
</code></pre>

<ul>
<li><pre><code> --pool)
</code></pre></li>
<li><pre><code> shift
</code></pre></li>
<li><pre><code> pool=$1
</code></pre></li>
<li><pre><code> shift
</code></pre></li>
<li><pre><code> ;;
</code></pre></li>
<li><pre><code> --rbd)
</code></pre></li>
<li><pre><code> shift
</code></pre></li>
<li><pre><code> rbd=$1
</code></pre></li>
<li><pre><code> shift
</code></pre></li>
<li><pre><code> ;;
--fstype)
shift
fstype=$1
</code></pre>

<p>@@ -161,7 +172,7 @@
fi</p></li>
</ul>


<p> case &ldquo;$backingstore&rdquo; in
&ndash;    lvm|none|btrfs|<em>unset) :;;
+    lvm|rbd|none|btrfs|</em>unset) :;;</p>

<pre><code> *) echo "'$backingstore' is not known ('none', 'lvm', 'btrfs')"
     usage
     exit 1
</code></pre>

<p>@@ -216,6 +227,13 @@</p>

<pre><code>     echo "please delete it (using \"lvremove $rootdev\") and try again"
     exit 1
 fi
</code></pre>

<p>+elif [ &ldquo;$backingstore&rdquo; = &ldquo;rbd&rdquo; ]; then
+    which rbd > /dev/null
+    if [ $? -ne 0 ]; then
+        echo &ldquo;rbd command not found. Please install ceph-common package&rdquo;
+        exit 1
+    fi
+    rootdev=/dev/rbd/$pool/$rbd
 elif [ &ldquo;$backingstore&rdquo; = &ldquo;btrfs&rdquo; ]; then</p>

<pre><code> mkdir "$lxc_path/$lxc_name"
 if ! out=$(btrfs subvolume create "$rootfs" 2&gt;&amp;1); then
</code></pre>

<p>@@ -257,6 +275,14 @@</p>

<pre><code> mkfs -t $fstype $rootdev || exit 1
 mount -t $fstype $rootdev $rootfs
</code></pre>

<p> fi
+
+if [ $backingstore = &ldquo;rbd&rdquo; ]; then
+    [ -d &ldquo;$rootfs&rdquo; ] || mkdir $rootfs
+    rbd create $pool/$rbd &mdash;size=$fssize || exit 1
+    rbd map $pool/$rbd || exit 1
+    mkfs -t $fstype $rootdev || exit 1
+    mount -t $fstype $rootdev $rootfs
+fi</p>

<p> if [ ! -z $lxc_template ]; then
```</p>

<p>If you want to make persistent after reboot, you must add rbd in /etc/ceph/rbdmap and add line in fstab.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RBD Replication]]></title>
    <link href="http://cephnotes.ksperis.com/blog/2014/08/12/rbd-replication/"/>
    <updated>2014-08-12T09:33:43+02:00</updated>
    <id>http://cephnotes.ksperis.com/blog/2014/08/12/rbd-replication</id>
    <content type="html"><![CDATA[<p>A simple exemple to make Replication for RBD.</p>

<!-- more -->


<p>Based on this post from scuttlemonkey : <a href="http://ceph.com/dev-notes/incremental-snapshots-with-rbd/,">http://ceph.com/dev-notes/incremental-snapshots-with-rbd/,</a> here is a sample script to synchronize rbd image on a remote cluster (eg for backups).
In the example below, the sync is made to an &ldquo;archive&rdquo; pool on the same cluster.
(For remote host, you need to use ssh key.)</p>

<p>``` bash</p>

<h1>!/bin/bash</h1>

<p>pool=&lsquo;rbd&rsquo;
pooldest=&lsquo;archive&rsquo;
rbd=&ldquo;myrbd&rdquo;
destination_host=&ldquo;127.0.0.1&rdquo;
snapname=&lsquo;rbd-sync-&rsquo;</p>

<h1>Retreive last synced id</h1>

<p>expr=&ldquo; $snapname([[:digit:]]+)&rdquo;
if rbd info $pool/$rbd >/dev/null 2>&1; then</p>

<pre><code>    rbd snap ls $pool/$rbd | grep "$expr" | sed  "s/.*$expr.*/\1/g" | sort -n &gt; /tmp/rbd-sync-snaplistlocal
</code></pre>

<p>else</p>

<pre><code>    echo "no image $pool/$rbd"
    return
</code></pre>

<p>fi
if ssh $destination_host rbd info $pooldest/$rbd >/dev/null 2>&1; then</p>

<pre><code>    ssh $destination_host rbd snap ls $pooldest/$rbd | grep "$expr" | sed "s/.*$expr.*/\1/g" | sort -n &gt; /tmp/rbd-sync-snaplistremote
</code></pre>

<p>else</p>

<pre><code>    echo "" &gt; /tmp/rbd-sync-snaplistremote
</code></pre>

<p>fi
syncid=$(comm -12 /tmp/rbd-sync-snaplistlocal /tmp/rbd-sync-snaplistremote | tail -n1)
lastid=$(cat /tmp/rbd-sync-snaplistlocal /tmp/rbd-sync-snaplistremote | sort -n | tail -n1)
nextid=$(($lastid + 1))</p>

<h1>Initial sync</h1>

<p>if [ &ldquo;$syncid&rdquo; = &ldquo;&rdquo; ]; then</p>

<pre><code>    echo "Initial sync with id $nextid"
    rbd snap create $pool/$rbd@$snapname$nextid
    rbd export --no-progress $pool/$rbd@$snapname$nextid - \
    | ssh $destination_host rbd import --image-format 2 - $pooldest/$rbd
    ssh $destination_host rbd snap create $pooldest/$rbd@$snapname$nextid
</code></pre>

<h1>Incremental sync</h1>

<p>else</p>

<pre><code>    echo "Found synced id : $syncid"
    rbd snap create $pool/$rbd@$snapname$nextid

    echo "Sync $syncid -&gt; $nextid"

    rbd export-diff --no-progress --from-snap $snapname$syncid $pool/$rbd@$snapname$nextid - \
    | ssh $destination_host rbd import-diff - $pooldest/$rbd
</code></pre>

<p>fi
```</p>

<p>An other exemple : <a href="https://www.rapide.nl/blog/item/ceph_-_rbd_replication">https://www.rapide.nl/blog/item/ceph_-_rbd_replication</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Low Cost Scale-Out Nas for the Office]]></title>
    <link href="http://cephnotes.ksperis.com/blog/2014/08/07/low-cost-scale-out-nas-for-the-office/"/>
    <updated>2014-08-07T22:30:00+02:00</updated>
    <id>http://cephnotes.ksperis.com/blog/2014/08/07/low-cost-scale-out-nas-for-the-office</id>
    <content type="html"><![CDATA[<p>The aim is showing that it is possible to create a low-cost storage, efficient and scalable, using opensource solutions.
In the example below, I am using <a href="http://ceph.com/">Ceph</a> for scalability and reliability, combined with <a href="https://github.com/stec-inc/EnhanceIO/">EnhanceIO</a> to ensure very good performance.</p>

<!-- more -->


<h2>Architecture</h2>

<p>The idea was to create a storage with two parts: the storage itself (backing store) and a large cache to keep good performance on all data actually used.
In fact, the volume needs may be important, but in the context of use for office, the data really used every day is only a portion of this storage.
In my case, I intend to use Ceph deployed on low-cost hardware to ensure a scalable and reliable conformatble volume, based on small servers with large SATA drives. Data access will be via <a href="http://www.samba.org/">Samba</a> shares on a slightly more powerful machine with an SAS Array to create a big cache. (Since Firefly, it would be more interesting to use ceph cache tiering)</p>

<p><img src="/images/img014.png"></p>

<h2>Hardware</h2>

<h4>Ceph Part (Backing Store)</h4>

<p>The material you choose should match the requirements of both performance and cost. To keep prices extremely competitive machines chosen here are based on Supermicro hardware without additional disk controller. Initially, the ceph storage is composed of 5 machines: store-01-02 store, store-03-04 store, store-05. Each node is build with simple CPU Core 2, 4 GB Memory, 1 SSD Drive (Intel 520 60GB) for system and Ceph journal and 3 SATA Drive of 3 TO (Seagate CS 3TB) with no specific Drive controller (Using onbord controller).</p>

<h4>Samba / EnhanceIO Part (File Server and Cache)</h4>

<p>The cache must be on the same server as the file server. In my case, I use Dell server with 10 sas drive 10k in Raid10 for the cache.</p>

<h4>Network</h4>

<p>I do not provide for a specific network, just a gigabit switch with vlan to separate public / private network for Ceph OSD and interface bonding for file server.</p>

<p>The system is currently used everyday since more than one year. And it works perfectly.</p>

<h1>Installation</h1>

<p>Here is some detail of the installation (from what I remember or I noted):</p>

<h2>Ceph Cluster</h2>

<p>The current configuration is running on Ceph Cuttlefish (v0.61) and Debian Wheezy. (Installation made in June 2013.)</p>

<h4>Debian install and partitionning</h4>

<p>The operating system is installed on the SSD with 3 others partitions for osd journal.</p>

<p><img src="/images/img001.png"></p>

<pre><code>$ apt-get install vim sudo sysstat ntp smartmontools
$ vim /etc/default/smartmontools
$ vim /etc/smartd.conf
</code></pre>

<h4>Kernel upgrade</h4>

<p>The use of the kernel version 3.12 available in backports seems improve memory footprint on the osd node.</p>

<pre><code>$ echo "deb http://ftp.fr.debian.org/debian wheezy-backports main" &gt;&gt; /etc/apt/sources.list
$ apt-get install -t wheezy-backports linux-image-3.12-0.bpo.1-amd64
</code></pre>

<h4>Ceph Installation</h4>

<p>( Based on the official documentation: <a href="http://ceph.com/docs/master/start/">http://ceph.com/docs/master/start/</a> )</p>

<p>On each server create a ceph user</p>

<pre><code>$ useradd -d /home/ceph -m ceph
$ passwd ceph

$ vim [[/etc/hosts]]
192.168.0.1       store-b1-01
192.168.0.2       store-b1-02
192.168.0.3       store-b1-03
192.168.0.4       store-b1-04
192.168.0.5       store-b1-05

$ echo "ceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
$ chmod 0440 /etc/sudoers.d/ceph
</code></pre>

<p>On store-b1-01 (deployment server)</p>

<p>Create a new key for ssh authentication :</p>

<pre><code>$ ssh-keygen

$ cluster="store-b1-01 store-b1-02 store-b1-03 store-b1-04 store-b1-05"
$ for i in $cluster; do
      ssh-copy-id ceph@$i
  done

$ vim /root/.ssh/config
Host store*
        User ceph
</code></pre>

<p>Install ceph-deploy and its dependencies</p>

<pre><code>$ wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
$ echo deb http://eu.ceph.com/debian-cuttlefish/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
$ apt-get update
$ apt-get install python-pkg-resources python-setuptools ceph-deploy collectd
$ curl http://python-distribute.org/distribute_setup.py | python
$ easy_install pushy
</code></pre>

<p>Install ceph on cluster</p>

<pre><code>$ ceph-deploy install $cluster
$ ceph-deploy new store-b1-01 store-b1-02 store-b1-03
$ vim ceph.conf
$ ceph-deploy mon create store-b1-01 store-b1-02 store-b1-03
</code></pre>

<p>Cluster deployment</p>

<pre><code>$ ceph-deploy gatherkeys store-b1-01
$ ceph-deploy osd create \
            store-b1-01:/dev/sdb:/dev/sda5 \
            store-b1-01:/dev/sdc:/dev/sda6 \
            store-b1-01:/dev/sdd:/dev/sda7 \
            store-b1-02:/dev/sdb:/dev/sda5 \
            store-b1-02:/dev/sdc:/dev/sda6 \
            store-b1-02:/dev/sdd:/dev/sda7 \
            store-b1-03:/dev/sdb:/dev/sda5 \
            store-b1-03:/dev/sdc:/dev/sda6 \
            store-b1-03:/dev/sdd:/dev/sda7 \
            store-b1-04:/dev/sdb:/dev/sda5 \
            store-b1-04:/dev/sdc:/dev/sda6 \
            store-b1-04:/dev/sdd:/dev/sda7 \
            store-b1-05:/dev/sdb:/dev/sda5 \
            store-b1-05:/dev/sdc:/dev/sda6 \
            store-b1-05:/dev/sdd:/dev/sda7
</code></pre>

<p>Add in fstab:</p>

<pre><code>/dev/sdb1       /var/lib/ceph/osd/ceph-0        xfs     inode64,noatime         0       0
/dev/sdc1       /var/lib/ceph/osd/ceph-1        xfs     inode64,noatime         0       0
/dev/sdd1       /var/lib/ceph/osd/ceph-2        xfs     inode64,noatime         0       0
</code></pre>

<p>One can check that all osd have been created, and check the cluster status :</p>

<pre><code>$ ceph osd tree

# id    weight  type name   up/down reweight
-1  41.23   root default
-8  41.23       datacenter DC1
-7  41.23           rack b1
-2  8.24                host store-b1-01
0   2.65                    osd.0   up  1   
1   2.65                    osd.1   up  1   
2   2.65                    osd.2   up  1   
-3  8.24                host store-b1-02
3   2.65                    osd.3   up  1   
4   2.65                    osd.4   up  1   
5   2.65                    osd.5   up  1   
-4  8.24                host store-b1-03
6   2.65                    osd.6   up  1   
7   2.65                    osd.7   up  1   
8   2.65                    osd.8   up  1   
-5  8.24                host store-b1-04
9   2.65                    osd.9   up  1   
10  2.65                    osd.10  up  1   
11  2.65                    osd.11  up  1   
-6  8.24                host store-b1-05
12  2.65                    osd.12  up  1   
13  2.65                    osd.13  up  1   
14  2.65                    osd.14  up  1   
</code></pre>

<h2>File Server</h2>

<h4>Debian interface bonding</h4>

<pre><code>$ apt-get install ifenslave

$ vim /etc/network/interface
auto bond0
iface bond0 inet static
        address 192.168.0.12
        netmask 255.255.0.0
        gateway 192.168.0.1
        slaves eth0 eth1
        bond-mode 802.3ad

$ echo "alias bond0 bonding
options bonding mode=4 miimon=100 lacp_rate=1" &gt; vim /etc/modprobe.d/bonding.conf

$ echo "bonding" &gt;&gt; /etc/modules
</code></pre>

<h4>Kernel Version</h4>

<p>The kernel version is important for using KRBD. I recommend to use at least kernel 3.10.26 or later.</p>

<pre><code>$ apt-get install debconf-utils dpkg-dev debhelper build-essential kernel-package libncurses5-dev
$ cd /usr/src/
$ wget http://www.kernel.org/pub/linux/kernel/v3.0/linux-3.6.11.tar.bz2
$ tar xjf linux-3.6.11.tar.bz2
</code></pre>

<h4>RBD</h4>

<p>Create rbd volume (Format 1) :</p>

<pre><code>$ rbd create datashare/share1 --image-format=1 --size=1048576

$ mkdir /share1
$ echo "/dev/rbd/datashare/share1   /share1 xfs _netdev,barrier=0,nodiratime        0   0" &gt;&gt; /etc/fstab
</code></pre>

<h4>EnhanceIO</h4>

<p>The choice of cache mechanism is focused on &ldquo;EnhanceIO&rdquo; because it allows to enable or disable cache while a source volume is being used. This is particularly useful when we want to resize a volume without interrupting service.</p>

<p>Build EnhanceIO :</p>

<pre><code>$ apt-get install build-essential dkms
$ git clone https://github.com/stec-inc/EnhanceIO.git
$ cd EnhanceIO/
$ wget http://ftp.de.debian.org/debian/pool/main/e/enhanceio/enhanceio_0+git20130620-3.debian.tar.xz
$ tar xJf enhanceio_0+git20130620-3.debian.tar.xz
$ dpkg-buildpackage -rfakeroot -b
$ dpkg -i ../*.deb
</code></pre>

<p>Create cache :</p>

<p>For exemple in write-though :
(/dev/sdb2 is a local partition dedicate for cache)</p>

<pre><code>$ eio_cli create -d /dev/rbd1 -s /dev/sdb2 -p lru -m wt -b 4096 -c share1
</code></pre>

<p>If you want to use write-back cache, you can protect the file system to mount before the cache by using a symbolic link in the udev script. ( <a href="https://github.com/ksperis/EnhanceIO/commit/954e167fdb580d514747512ce2bd1c9c29a77418">https://github.com/ksperis/EnhanceIO/commit/954e167fdb580d514747512ce2bd1c9c29a77418</a> )</p>

<h4>Samba</h4>

<p>Packages installation</p>

<pre><code>$ echo "deb http://ftp.sernet.de/pub/samba/3.6/debian wheezy main" &gt;&gt; /etc/apt/sources.list
$ apt-get update
$ apt-get install sernet-samba sernet-winbind xfsprogs krb5-user acl attr
</code></pre>

<p>Update startup script</p>

<pre><code>$ vim /etc/init.d/samba
# Should-Start:      slapd cups rbdmap
# Should-Stop:       slapd cups rbdmap

$ insserv -d samba
</code></pre>

<p>Configure and join the domain ( <a href="https://help.ubuntu.com/community/ActiveDirectoryWinbindHowto">https://help.ubuntu.com/community/ActiveDirectoryWinbindHowto</a> )</p>

<pre><code>$ vi /etc/krb5.conf
...
$ kinit Administrator@AD.MYDOMAIN.COM

$ vim /etc/samba/smb.conf
[global]
    workgroup = MYDOMAIN
    realm = AD.MYDOMAIN.COM
    netbios name = MYNAS
    wins server = 192.168.0.4
    server string = %h server
    dns proxy = no
    log file = /var/log/samba/log.%m
    log level = 1
    max log size = 1000
    syslog = 0
    panic action = /usr/share/samba/panic-action %d
    security = ADS
    winbind separator = +
    client use spnego = yes
    winbind use default domain = yes
    domain master = no
    local master = no
    preferred master = no
    encrypt passwords = true
    passdb backend = tdbsam
    obey pam restrictions = yes
    unix password sync = yes
    passwd program = /usr/bin/passwd %u
    passwd chat = *Enter\snew\s*\spassword:* %n\n *Retype\snew\s*\spassword:* %n\n *password\supdated\ssuccessfully* .
    pam password change = yes
    idmap uid = 10000-20000
    idmap gid = 10000-20000
    template shell = /bin/bash
    template homedir = /share4/home/%D/%U
    winbind enum groups = yes
    winbind enum users = yes
    map acl inherit = yes
    vfs objects = acl_xattr recycle shadow_copy2
    recycle:repository =.recycle/%u 
    recycle:keeptree = yes
    recycle:exclude = *.tmp
    recycle:touch = yes
    shadow:snapdir = .snapshots
    shadow:sort = desc
    ea support = yes
    map hidden = no
    map system = no
    map archive = no
    map readonly = no
    store dos attributes = yes
    load printers = no
    printing=bsd
    printcap name = /dev/null
    disable spoolss = yes
    guest account = invité
    map to guest = bad user

$================================================================

[share0]
    comment = My first share
    path = /share0
    writable = yes
    valid users = @"MYDOMAIN+Domain Admins" "MYDOMAIN+laurent"

[share1]
    comment = Other share
    path = /share1
    writable = yes
    valid users = @"MYDOMAIN+Domain Admins" "MYDOMAIN+laurent"

....


$ /etc/init.d/samba restart
$ net join -U Administrator
$ wbinfo -u
$ wbinfo -g

$ vi /etc/nsswitch.conf
passwd:         compat winbind
group:          compat winbind
</code></pre>

<h4>Virtual Shadow Copy</h4>

<p>Using this script :
<a href="https://github.com/ksperis/autosnap-rbd-shadow-copy">https://github.com/ksperis/autosnap-rbd-shadow-copy</a></p>

<h4>TimeMachine Backup</h4>

<p>Fisrt, create rbd timemachine and mount to /mnt/timemachine with noatime and uquota if you want to use per user quota (Do not add to the autosnap script.)</p>

<p>Install avahi</p>

<pre><code>$ apt-get install avahi-daemon avahi-utils

$ vim /etc/avahi/services/smb.service
&lt;?xml version="1.0" standalone='no'?&gt;
&lt;!DOCTYPE service-group SYSTEM "avahi-service.dtd"&gt;
&lt;service-group&gt;
  &lt;name replace-wildcards="yes"&gt;%h (File Server)&lt;/name&gt;
  &lt;service&gt;
    &lt;type&gt;_smb._tcp&lt;/type&gt;
    &lt;port&gt;445&lt;/port&gt;
  &lt;/service&gt;
  &lt;service&gt;
    &lt;type&gt;_device-info._tcp&lt;/type&gt;
    &lt;port&gt;0&lt;/port&gt;
    &lt;txt-record&gt;model=RackMac&lt;/txt-record&gt;
  &lt;/service&gt;
&lt;/service-group&gt;

$ vim /etc/avahi/services/timemachine.service
&lt;?xml version="1.0" standalone="no"?&gt;
&lt;!DOCTYPE service-group SYSTEM "avahi-service.dtd"&gt;
&lt;service-group&gt;
  &lt;name replace-wildcards="yes"&gt;%h (Time Machine)&lt;/name&gt;
  &lt;service&gt;
    &lt;type&gt;_afpovertcp._tcp&lt;/type&gt;
    &lt;port&gt;548&lt;/port&gt;
  &lt;/service&gt;
  &lt;service&gt;
    &lt;type&gt;_device-info._tcp&lt;/type&gt;
    &lt;port&gt;0&lt;/port&gt;
    &lt;txt-record&gt;model=TimeCapsule&lt;/txt-record&gt;
  &lt;/service&gt;
  &lt;service&gt;
    &lt;type&gt;_adisk._tcp&lt;/type&gt;
    &lt;port&gt;9&lt;/port&gt;
    &lt;txt-record&gt;sys=waMA=00:1d:09:63:87:e0,adVF=0x100&lt;/txt-record&gt;
    &lt;txt-record&gt;dk0=adVF=0x83,adVN=TimeMachine&lt;/txt-record&gt;
  &lt;/service&gt;
&lt;/service-group&gt;
</code></pre>

<p>Install netatalk and configure like this :</p>

<pre><code>$ apt-get install netatalk
$ vim /etc/netatalk/afpd.conf
- -tcp -noddp -nozeroconf -uamlist uams_dhx.so,uams_dhx2.so -nosavepassword -setuplog "default log_info /var/log/afpd.log"


$ vim /etc/netatalk/AppleVolumes.default   (Remove home directory + Add :)
/mnt/timemachine/ "TimeMachine" cnidscheme:dbd options:usedots,upriv,tm allow:@"MYDOMAIN+Domain Users"

$ vim /etc/pam.d/netatalk
auth       required     pam_winbind.so
account    required     pam_winbind.so
session    required     pam_unix.so
</code></pre>

<p>Setting quota for a user : (Do not use soft limits, because it will not be recognized by timemachine.)</p>

<pre><code>$ xfs_quota -x -c 'limit bhard=1024g user1' /mnt/timemachine
</code></pre>

<h2>Benchmark</h2>

<p><img src="/images/img013.png"></p>

<p>( For sequential IO, bandwidth is limited by the network card on the client side. )</p>
]]></content>
  </entry>
  
</feed>
